{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This Program Downloads the historical stock price and index data from sharesansar.com and merges to the existing excel csv file. The program automatically identifies the most recent available date and downloads the remaining data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking For Latest Available Data on PriceHistory.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv(r'PriceHistory.csv', parse_dates= ['Date'],  thousands= ',')\n",
    "data.sort_values(by = 'Date', ascending = True, inplace = True)\n",
    "latest_data_available = data['Date'].iloc[-1]\n",
    "print(latest_data_available.date())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date To Scrape From Sharesansar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "date_to_start_scraping_from  = latest_data_available.date() + pd.to_timedelta(1, unit=\"D\")\n",
    "dates = pd.date_range(start = date_to_start_scraping_from, end = pd.to_datetime(\"today\"))\n",
    "dates = [d for d in dates if not d.isoweekday() in [5,6]] # List excluding Friday and Saturday\n",
    "print(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import time\n",
    "import os\n",
    "import calendar\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"start-maximized\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "driver.get('https://www.sharesansar.com/today-share-price')\n",
    "\n",
    "blank_df = []\n",
    "for date in dates:\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"fromdate\"]').clear()\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"fromdate\"]').send_keys(f'{date.date()}') \n",
    "    driver.find_element(By.XPATH,'//*[@id=\"btn_todayshareprice_submit\"]').click()\n",
    "    time.sleep(10)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    todays_price = soup.find('table', id = 'headFixed')\n",
    "    if len(todays_price.findAll('tr'))<10:\n",
    "        pass\n",
    "    else:\n",
    "        output_rows = []\n",
    "        for table_row in todays_price.findAll('tr'): # looping through all the table rows in a page\n",
    "            columns = table_row.findAll('td') #finding the cell values of every row in the table\n",
    "            output_row = []\n",
    "            for column in columns: # looping through each cellvalue data in a row \n",
    "                output_row.append(column.text)\n",
    "            output_rows.append(output_row)\n",
    "            headers_list = []\n",
    "            for headers in todays_price.find_all('th'):\n",
    "                headers_list.append(headers.text)\n",
    "        todays_price_dataframe = pd.DataFrame(output_rows)[1:]\n",
    "        todays_price_dataframe.columns = headers_list\n",
    "        todays_price_dataframe['Symbol'] = todays_price_dataframe['Symbol'].str.replace(\"\\n\", \"\")\n",
    "        todays_price_dataframe.set_index('S.No', inplace = True)\n",
    "        todays_price_dataframe['Date'] = date.date()\n",
    "        blank_df.append(todays_price_dataframe)\n",
    "        print(date.date())\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.concat(blank_df)\n",
    "df['Date']  = df['Date'].astype('datetime64')\n",
    "df.set_index('Date', inplace = True)\n",
    "df = df[['Symbol','Open','High', 'Low', 'Close','Vol', 'Turnover','Trans.']]\n",
    "columns_except_symbol = df.columns.difference(['Symbol'])\n",
    "df[columns_except_symbol]= df[columns_except_symbol].replace(',',  '', regex= True).astype(float)\n",
    "last_traded_day = df.index[-1]\n",
    "print(last_traded_day.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('PriceHistory.csv', 'a') as f:\n",
    "    df.to_csv(f, header = False, lineterminator='\\n')\n",
    "print('Completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Index From Sharesansar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharesansar_sectors= ['Nepse Index','Banking SubIndex',\t'Development Bank Index',\t'Finance Index',\t'Hotels And Tourism',\t'HydroPower Index',\t'Investment',\t'Life Insurance',\t'Manufacturing And Processing',\t'Microfinance Index',\t'Mutual Fund',\t'Non Life Insurance',\t'Others Index',\t'Trading Index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import time\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"start-maximized\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "driver.get('https://www.sharesansar.com/index-history-data')\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "\n",
    "driver.find_element(By.ID,'fromDate').clear()\n",
    "driver.find_element(By.ID,\"fromDate\").send_keys(f\"{dates[0].date()}\") \n",
    "driver.find_element(By.ID,'toDate').clear()\n",
    "driver.find_element(By.ID,'toDate').send_keys(f'{last_traded_day.date()}') #f'{last_traded_day.date()}'\n",
    "\n",
    "\n",
    "\n",
    "sectors_df = []\n",
    "for each_sector in sharesansar_sectors:\n",
    "    \n",
    "    driver.find_element(By.ID,\"select2-index-container\").click()\n",
    "    driver.find_element(By.CSS_SELECTOR,\"input[class='select2-search__field']\").send_keys(each_sector)\n",
    "\n",
    "    driver.find_element(By.ID, \"btn_indxhis_submit\").click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    output_rows = []\n",
    "    for table_row in soup.findAll('tr'): # looping through all the table rows in a page\n",
    "        columns = table_row.findAll('td') #finding the cell values of every row in the table\n",
    "        output_row = []\n",
    "        for column in columns: # looping through each cellvalue data in a row \n",
    "            output_row.append(column.text)\n",
    "        output_rows.append(output_row)\n",
    "        headers_list = []\n",
    "        for headers in soup.find_all('th'):\n",
    "            headers_list.append(headers.text)\n",
    "\n",
    "    index_data = pd.DataFrame(output_rows)[1:]\n",
    "    index_data.columns = headers_list\n",
    "    index_data.insert(loc = 0, column = 'Ticker', value  = each_sector)\n",
    "    index_data['Date']  = index_data['Date'].astype('datetime64')\n",
    "    index_data.set_index('Date', inplace = True)\n",
    "    columns_except_symbol = index_data.columns.difference(['Ticker'])\n",
    "    index_data[columns_except_symbol]= index_data[columns_except_symbol].replace(',',  '', regex= True).astype(float)\n",
    "    index_data = index_data.drop(['S.N.', 'Change', 'Per Change (%)'], axis = 1)\n",
    "    time.sleep(1)\n",
    "    sectors_df.append(index_data)\n",
    "    print(each_sector)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_indices_data = pd.concat(sectors_df)\n",
    "with open('PriceHistory.csv', 'a') as f:\n",
    "    daily_indices_data.to_csv(f, header = False, lineterminator='\\n')\n",
    "print('*Completed*')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
